{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrsaDfdVHzxt"
      },
      "source": [
        "# Custom Training with YOLOv5\n",
        "\n",
        "We assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
        "\n",
        "* Gather a dataset of images and label our dataset\n",
        "* Export our dataset to YOLOv5\n",
        "* Train YOLOv5 to recognize the objects in our dataset\n",
        "* Evaluate our YOLOv5 model's performance\n",
        "* Run test inference to view our model at work\n",
        "\n",
        "\n",
        "\n",
        "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTvDNSILZoN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3b04c9-1014-4ff7-a332-d32aac6b9e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15679, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 15679 (delta 1), reused 2 (delta 0), pack-reused 15672\u001b[K\n",
            "Receiving objects: 100% (15679/15679), 14.45 MiB | 18.27 MiB/s, done.\n",
            "Resolving deltas: 100% (10742/10742), done.\n",
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.5/586.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Setup complete. Using torch 2.0.1+cu118 (CPU)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6USLgz2f0r"
      },
      "source": [
        "# Step 2: Assemble Our Dataset\n",
        "\n",
        "In order to train our custom model, we need to assemble a dataset of representative images with bounding box annotations around the objects that we want to detect. And we need our dataset to be in YOLOv5 format.\n",
        "\n",
        "In Roboflow, you can choose between two paths:\n",
        "\n",
        "* Convert an existing dataset to YOLOv5 format. Roboflow supports over [30 formats object detection formats](https://roboflow.com/formats) for conversion.\n",
        "* Upload raw images and annotate them in Roboflow with [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
        "\n",
        "# Annotate\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)\n",
        "\n",
        "# Version\n",
        "\n",
        "![](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIF6oKijdOK5"
      },
      "source": [
        "##Setting Up Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jjT5uIHo6l5"
      },
      "outputs": [],
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZUIkNGEeKoD"
      },
      "source": [
        "##Cloning Dataset From Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqm87OjSdBAb",
        "outputId": "4b0c62b8-37b0-4003-f1ae-46b5eb1ea402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YOLOv5-ObjectDetection'...\n",
            "remote: Enumerating objects: 12510, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 12510 (delta 1), reused 0 (delta 0), pack-reused 12505\u001b[K\n",
            "Receiving objects: 100% (12510/12510), 297.92 MiB | 20.62 MiB/s, done.\n",
            "Resolving deltas: 100% (483/483), done.\n",
            "Updating files: 100% (7507/7507), done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# contains modified versions of detect.py, data.yaml\n",
        "# also contains best.pt !!!\n",
        "!git clone https://github.com/Ojus999/YOLOv5-ObjectDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yJGIWwNedu-"
      },
      "outputs": [],
      "source": [
        "# setting path to the newly cloned repo\n",
        "PATH = \"/content/yolov5/YOLOv5-ObjectDetection\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3faeJluvedLx"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# Step 3: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaFNnxLJbq4J"
      },
      "outputs": [],
      "source": [
        "# img arg was 416\n",
        "# not required as best.pt is there in cloned repo\n",
        "# !python train.py --img 472 --batch 16 --epochs 50 --data {PATH}/data.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n",
        "\n",
        "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y09WUNlxXWko"
      },
      "outputs": [],
      "source": [
        "# killing to prevent hanging\n",
        "!kill 3396"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jS9_BxdBBHL"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "# not possible  as we have not trained\n",
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Folder for Cropped Images\n"
      ],
      "metadata": {
        "id": "HUDnIK5oUEaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/yolov5/Cropped_Imgs'):\n",
        "  !mkdir {'/content/yolov5/Cropped_Imgs'}\n",
        "\n",
        "if not os.path.exists('/content/yolov5/Cropped_Imgs/License'):\n",
        "  !mkdir {'/content/yolov5/Cropped_Imgs/License'}\n",
        "\n",
        "if not os.path.exists('/content/yolov5/Cropped_Imgs/Face'):\n",
        "  !mkdir {'/content/yolov5/Cropped_Imgs/Face'}"
      ],
      "metadata": {
        "id": "jsF4RhfwUEED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deleting Files in Cropped Images"
      ],
      "metadata": {
        "id": "46VnblMXUXzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('/content/yolov5/Cropped_Imgs/Face'):\n",
        "  Path = '/content/yolov5/Cropped_Imgs/Face'\n",
        "  for file in os.listdir(Path):\n",
        "    os.unlink(os.path.join(Path,file))\n",
        "\n",
        "if os.path.exists('/content/yolov5/Cropped_Imgs/License'):\n",
        "  Path = '/content/yolov5/Cropped_Imgs/License'\n",
        "  for file in os.listdir(Path):\n",
        "    os.unlink(os.path.join(Path,file))"
      ],
      "metadata": {
        "id": "JFactHWwUBda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Move Modified Detect.py To The Required Folder"
      ],
      "metadata": {
        "id": "ib7Ilv9a0e-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "src_path = \"/content/yolov5/YOLOv5-ObjectDetection/detect-1.py\"\n",
        "dst_path = \"/content/yolov5/detect-1.py\"\n",
        "shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q18B-_ah0k3b",
        "outputId": "ed7d169f-ad9d-41a8-bf4e-126c596a2f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/yolov5/detect-1.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWjjiBcic3Vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988757f0-a93a-4848-accc-773bbf02b40d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect-1: \u001b[0mweights=['/content/yolov5/YOLOv5-ObjectDetection/best.pt'], source=/content/yolov5/YOLOv5-ObjectDetection/test/images, data=data/coco128.yaml, imgsz=[416, 416], conf_thres=0.7, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /usr/local/lib/python3.10/dist-packages/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 v7.0-168-gec2b853 Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/-I1-MS09uaqsLdGTFkgnS0Rcg1mmPyAj95ySg_eckoM_jpeg_jpg.rf.ca3d216ca28be32265e365299a0af4b6.jpg: 416x416 2 faces, 7.5ms\n",
            "image 2/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/0ad90195-cd77-489e-bf85-08c83b80d3e0_jpg.rf.3317b17670bf4318a3b12a9805730774.jpg: 416x416 3 faces, 8.5ms\n",
            "image 3/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/1125506397_15801322206131n_jpg.rf.e15d50e045c169dc1c03cb1c926f90b9.jpg: 416x416 7 faces, 7.4ms\n",
            "image 4/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/1196686205_jpg_14_jpg.rf.70c3ce46b74cee515e2d37cde8fc0202.jpg: 416x416 3 faces, 7.5ms\n",
            "image 5/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/1288788-une-employee-aide-des-voyageurs-en-provenance-de-chine-le-26-janvier-2020-a-l-aeroport-de-roissy_jpg.rf.55458647417fc55d575967c5ef01a74f.jpg: 416x416 4 faces, 7.4ms\n",
            "image 6/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/1422808187-3291816118_jpg.rf.d732e1de64322f02b57f00aa405323e3.jpg: 416x416 1 face, 7.5ms\n",
            "image 7/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/1482202839575_jpg.rf.b9d0c8f35100355b00e945fd04c39c3f.jpg: 416x416 4 faces, 7.4ms\n",
            "image 8/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/1483800496-3386248642_jpg.rf.f21f61bbc9340ac4e346ac1d1f2817f8.jpg: 416x416 3 faces, 7.6ms\n",
            "image 9/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/15-08608-001_jpg.rf.a908e7d5b9da2ffe5560973f95a3527c.jpg: 416x416 4 faces, 7.4ms\n",
            "image 10/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/1553605632_9d5877d8_60_jpg.rf.fdebf5a49772d5b48787c690b5048962.jpg: 416x416 7 faces, 7.5ms\n",
            "image 11/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/20221109_164231_jpg.rf.b9ea4fb106ad4a567ad305c8a2bfb60a.jpg: 416x416 (no detections), 7.5ms\n",
            "image 12/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/20221109_164330_jpg.rf.b596ea818a9df35e857908ead69f024c.jpg: 416x416 (no detections), 7.4ms\n",
            "image 13/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/20221109_164420_jpg.rf.2f04182807bff65c25cdc0a563f78a7d.jpg: 416x416 (no detections), 7.4ms\n",
            "image 14/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/20221109_164524-1-_jpg.rf.921b838f2d90f891c4c5172d2d446b4b.jpg: 416x416 (no detections), 7.4ms\n",
            "image 15/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/20221109_164558_jpg.rf.b3c826a1f856d253a0b04ca691c20a32.jpg: 416x416 (no detections), 7.5ms\n",
            "image 16/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/20221109_165214_jpg.rf.95c60d9b0b2566fceceff03dd7044394.jpg: 416x416 (no detections), 11.3ms\n",
            "image 17/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/2559_jpeg.rf.46b963c9edce17327a5d29fd8ccf1134.jpg: 416x416 5 faces, 8.0ms\n",
            "image 18/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars0_png.rf.20f78c5916b78e5227b41cbb718b07db.jpg: 416x416 1 license, 9.2ms\n",
            "image 19/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars10_png.rf.adfaf69c3a281a408eaa7c30dffbbe30.jpg: 416x416 (no detections), 7.4ms\n",
            "image 20/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars11_png.rf.4d2d5ee66fc2bb426a7682c7e089d28a.jpg: 416x416 1 license, 7.4ms\n",
            "image 21/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars1_png.rf.c7ed6303281fb84d22ed40f104d7d7f5.jpg: 416x416 1 license, 7.4ms\n",
            "image 22/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars200_png.rf.0077bc95f6a2f54343b18f278f4c7fb2.jpg: 416x416 1 license, 7.5ms\n",
            "image 23/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars201_png.rf.0c61a77edbd52e2ed623d79999738823.jpg: 416x416 1 license, 9.1ms\n",
            "image 24/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars202_png.rf.4b5ebd748c52ab3ef6d3fe5e410328c6.jpg: 416x416 1 license, 8.8ms\n",
            "image 25/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars203_png.rf.a6e496e47c902bad677d5289b207e7d8.jpg: 416x416 1 license, 7.4ms\n",
            "image 26/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars204_png.rf.a66390e65030e4d529dc285af9c58568.jpg: 416x416 1 license, 7.4ms\n",
            "image 27/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars205_png.rf.36f6d981af1f1a482228b611986f9929.jpg: 416x416 1 license, 8.4ms\n",
            "image 28/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars206_png.rf.ba6e6ef0f180785027a082f2ae53cca2.jpg: 416x416 1 license, 7.4ms\n",
            "image 29/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars225_png.rf.b6256875c826d13fe98c4016cd13cd6d.jpg: 416x416 1 license, 7.7ms\n",
            "image 30/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars2_png.rf.07e29883eb09ceffd4971257eb084b2f.jpg: 416x416 1 license, 7.8ms\n",
            "image 31/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars3_png.rf.db05e99d30b090ea3cb3e048fff5bcd8.jpg: 416x416 1 license, 10.0ms\n",
            "image 32/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars4_png.rf.fe59dcf5c8514a437bc5b8d625ac3bc9.jpg: 416x416 (no detections), 9.2ms\n",
            "image 33/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars6_png.rf.27ee57cde179f0fb21027ca7dd89edf9.jpg: 416x416 1 license, 7.4ms\n",
            "image 34/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars7_png.rf.c7d55259a47ae60fa2e2bc5cfa34396a.jpg: 416x416 1 license, 7.4ms\n",
            "image 35/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars8_png.rf.184303eddb67fda3eaae5ea9f2ad2e2f.jpg: 416x416 1 license, 8.8ms\n",
            "image 36/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Cars9_png.rf.0b008532e7979184a5d15d7fc793a55c.jpg: 416x416 1 license, 7.7ms\n",
            "image 37/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4920_mov-12_jpg.rf.b1303aade98b1060acc6d1810aa71354.jpg: 416x416 1 face, 9.3ms\n",
            "image 38/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4920_mov-3_jpg.rf.0fa45ad0a4de14e1562e50d434a3ab47.jpg: 416x416 1 face, 9.3ms\n",
            "image 39/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-103_jpg.rf.a9b7f68eb84f0ffee9bdc886e5557cd1.jpg: 416x416 1 face, 9.9ms\n",
            "image 40/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-107_jpg.rf.fad409a4b4712e207998c19b8ac8ffe9.jpg: 416x416 1 face, 8.9ms\n",
            "image 41/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-113_jpg.rf.493ecd789ceab4e7e6fd8c04e5b2eea1.jpg: 416x416 1 face, 9.2ms\n",
            "image 42/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-127_jpg.rf.bdf9cb24b2784ef10eee23abc1242037.jpg: 416x416 1 face, 8.8ms\n",
            "image 43/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-24_jpg.rf.629eef3fb903e4492066e4176325424d.jpg: 416x416 1 face, 8.0ms\n",
            "image 44/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-31_jpg.rf.8cad07da7d21ec7b28137e52d73de170.jpg: 416x416 1 face, 8.8ms\n",
            "image 45/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-41_jpg.rf.c8a0e54a949a23e0a1eb1c7f0e51971e.jpg: 416x416 1 face, 8.7ms\n",
            "image 46/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-57_jpg.rf.401e129cde61038969828eb030370039.jpg: 416x416 1 face, 8.2ms\n",
            "image 47/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-5_jpg.rf.ddcd67602e5140c2ad57bf3862967787.jpg: 416x416 1 face, 8.6ms\n",
            "image 48/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-88_jpg.rf.b0e71dce67f722ccc5ba43a798085992.jpg: 416x416 1 face, 8.2ms\n",
            "image 49/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_4921-2_mp4-97_jpg.rf.cd5e6432a2ac0424154bc2ee3cd9eb7c.jpg: 416x416 1 face, 7.7ms\n",
            "image 50/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_5490_mp4-11_jpg.rf.86d40023b737c3a327232b0f057a62a4.jpg: 416x416 (no detections), 7.4ms\n",
            "image 51/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_5490_mp4-19_jpg.rf.c1862ae9d6b8d462cf6e166c98c10eca.jpg: 416x416 (no detections), 7.4ms\n",
            "image 52/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_5490_mp4-1_jpg.rf.ac7f26f9032fc24028fed43bd714ab51.jpg: 416x416 (no detections), 7.4ms\n",
            "image 53/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_5491_mp4-3_jpg.rf.c622260d3c4130bc1f465bee974403ff.jpg: 416x416 (no detections), 7.4ms\n",
            "image 54/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/IMG_5491_mp4-8_jpg.rf.024e19276b9a41566175f13c60c9857b.jpg: 416x416 (no detections), 7.4ms\n",
            "image 55/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Image-from-iOS_MOV-31_jpg.rf.6726561f613f63559773497b6c2626e0.jpg: 416x416 1 face, 7.4ms\n",
            "image 56/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Inside-merge_mov-12_jpg.rf.a70c50e61ad64703ccdad1e45f957265.jpg: 416x416 (no detections), 7.7ms\n",
            "image 57/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Inside-merge_mov-21_jpg.rf.9c70d3d873ec5e97db1c117ae7b93a89.jpg: 416x416 (no detections), 7.4ms\n",
            "image 58/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Inside-merge_mov-27_jpg.rf.a78fd81c30722372f79f40a801dd8269.jpg: 416x416 (no detections), 7.4ms\n",
            "image 59/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Inside-merge_mov-30_jpg.rf.3f216de3daaff897e23a7474626d0770.jpg: 416x416 (no detections), 7.4ms\n",
            "image 60/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Inside-merge_mov-58_jpg.rf.4a05351fcc3b5dd356dc1b3a5721727b.jpg: 416x416 (no detections), 7.4ms\n",
            "image 61/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/MTraore-photo_JPG.rf.ecb36e77faba2435208a3edd38d256e7.jpg: 416x416 1 face, 7.4ms\n",
            "image 62/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask-detector1_mov-14_jpg.rf.a87bb3080366eee10113034a609c16de.jpg: 416x416 1 face, 7.5ms\n",
            "image 63/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask-detector1_mov-23_jpg.rf.982232908c039952b50134a1f2ec5382.jpg: 416x416 1 face, 7.4ms\n",
            "image 64/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask-detector1_mov-27_jpg.rf.6efecd11b441599007cc57ea95e04d0d.jpg: 416x416 1 face, 7.4ms\n",
            "image 65/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask-detector1_mov-53_jpg.rf.cae536c80993053afa3a001d8f1a44eb.jpg: 416x416 1 face, 7.4ms\n",
            "image 66/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask2_mov-19_jpg.rf.cf4b2629d3a085fe428d83f9fb7bcfb6.jpg: 416x416 1 face, 7.4ms\n",
            "image 67/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask2_mov-38_jpg.rf.bcef090a8ffbdbc2fc3957e46b08ff5b.jpg: 416x416 1 face, 7.4ms\n",
            "image 68/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask2_mov-52_jpg.rf.3942d9c0f13e2b74285674078fcd0c3a.jpg: 416x416 1 face, 7.4ms\n",
            "image 69/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask2_mov-53_jpg.rf.3c49092cd804791b6843660ab3338200.jpg: 416x416 1 face, 7.4ms\n",
            "image 70/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask2_mov-56_jpg.rf.75acdf7e324b4a6a47f73f0817163503.jpg: 416x416 1 face, 7.4ms\n",
            "image 71/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mask2_mov-62_jpg.rf.53533d91e1a32019cb93de590330418d.jpg: 416x416 1 face, 7.4ms\n",
            "image 72/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mat-Hand-Signs_mov-128_jpg.rf.f306990032c27cd1c120ff06ef31f167.jpg: 416x416 1 face, 7.5ms\n",
            "image 73/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mat-Hand-Signs_mov-39_jpg.rf.5a63b4e9fca8e5c15294ab09328556bf.jpg: 416x416 1 face, 7.4ms\n",
            "image 74/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mohamed-Image-from-iOS_MOV-113_jpg.rf.ad3e6f890ddfbe7005f8ff4361833127.jpg: 416x416 1 face, 7.4ms\n",
            "image 75/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Mohamed-Image-from-iOS_MOV-26_jpg.rf.a743dc6053bc6f768cdec9a8cb343a77.jpg: 416x416 1 face, 7.4ms\n",
            "image 76/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/PXL_20210930_151849553_jpg.rf.2f57833c204616f38be742370a92800d.jpg: 416x416 1 face, 7.5ms\n",
            "image 77/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/PXL_20210930_151852090_jpg.rf.2b67442f4f70488b0dd0c79074273330.jpg: 416x416 1 face, 7.4ms\n",
            "image 78/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/PXL_20210930_151854592_jpg.rf.ec76f123d63d249d79344dc41cd020ea.jpg: 416x416 1 face, 7.4ms\n",
            "image 79/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/PXL_20210930_152055464_jpg.rf.72869ba2f8ccbd973a73f5c0bb993f36.jpg: 416x416 1 face, 7.4ms\n",
            "image 80/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Photo-on-10-26-21-at-2-33-PM-4-2_jpg.rf.d626db375271ddd01f74c1d7841471d1.jpg: 416x416 (no detections), 7.4ms\n",
            "image 81/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Photo-on-10-26-21-at-2-33-PM_jpg.rf.9bec10a23961bf1048e7b9792d2b02ba.jpg: 416x416 1 face, 7.4ms\n",
            "image 82/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Screen-Recording-2021-12-09-at-1_03_04-PM_mov-36_jpg.rf.7c446e7e2c6289b6f415414f8a1157a7.jpg: 416x416 1 face, 9.5ms\n",
            "image 83/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Screen-Recording-2021-12-09-at-1_03_04-PM_mov-3_jpg.rf.835b8e3c252217c0099b46afdd37d3b4.jpg: 416x416 1 face, 7.4ms\n",
            "image 84/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Screen-Recording-2021-12-09-at-1_03_04-PM_mov-96_jpg.rf.f40e4635ffd6219256e17b3db5ea5c97.jpg: 416x416 1 face, 7.4ms\n",
            "image 85/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/Stokoe20-58-scaled-1_jpg.rf.eb35cea43eba43738a7894a7a1c702dc.jpg: 416x416 3 faces, 7.4ms\n",
            "image 86/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/ashton-video_mov-55_jpg.rf.c9f962e3cc5ed2bfb08e4bf70b64f036.jpg: 416x416 1 face, 7.4ms\n",
            "image 87/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/cf3f907d-052f-443d-8c04-3cc992b41147_jpg.rf.4698bc98ef8a8501da603fb0da358766.jpg: 416x416 1 license, 7.4ms\n",
            "image 88/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/cf8a7503-c231-468b-a082-2ece747e878a_jpg.rf.fd9364b42c5c2347a8d41e626b4f2696.jpg: 416x416 1 license, 7.4ms\n",
            "image 89/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/cfa39731-cca0-48c3-8dfa-61008d591726_jpg.rf.5e30a5fb149be3211f3b1ff5ebc37a5d.jpg: 416x416 1 license, 7.4ms\n",
            "image 90/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/cfbdb782-ad7b-47d9-90ff-b0a4bc16477c_jpg.rf.ec9814681f2ee058969c869da01e43fd.jpg: 416x416 1 license, 7.4ms\n",
            "image 91/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/cfc0a325-1ab1-4dd0-a02f-1a1144b90de4_jpg.rf.9e67129dba295062889ff6a51d9922df.jpg: 416x416 1 license, 7.4ms\n",
            "image 92/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/download_jpeg_jpg.rf.a4e0e4736841e8dfc935c790f5195ae7.jpg: 416x416 1 face, 7.4ms\n",
            "image 93/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/face-detection_mp4-25_jpg.rf.f8dd835c93dd70bf10be06d8bd4a98ec.jpg: 416x416 1 face, 7.4ms\n",
            "image 94/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/face-detection_mp4-2_jpg.rf.4ecab99213311bf92827e04ff6507cba.jpg: 416x416 1 face, 7.4ms\n",
            "image 95/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/face-detection_mp4-32_jpg.rf.3399bd9bd347731844d18f131d9781a7.jpg: 416x416 1 face, 7.4ms\n",
            "image 96/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/face-detection_mp4-4_jpg.rf.de267863a0836f6bf7e92beaaead372e.jpg: 416x416 1 face, 7.4ms\n",
            "image 97/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-102_jpg.rf.a929532f15cc203ee948007330368bef.jpg: 416x416 1 face, 7.4ms\n",
            "image 98/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-118_jpg.rf.99efb242170bd6c6da83442697627470.jpg: 416x416 1 face, 7.4ms\n",
            "image 99/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-122_jpg.rf.48f2f25a31bd1f81657a0c9903a84efe.jpg: 416x416 1 face, 7.4ms\n",
            "image 100/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-126_jpg.rf.fdaf22d02b7ad71e4554578c2bb621ef.jpg: 416x416 (no detections), 7.4ms\n",
            "image 101/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-135_jpg.rf.c50c5d95d573de49b57bf0fdeccd7024.jpg: 416x416 1 face, 7.4ms\n",
            "image 102/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-13_jpg.rf.bd7e6e20d4897797c3594be39478ffc0.jpg: 416x416 2 faces, 7.4ms\n",
            "image 103/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-143_jpg.rf.2fb8124dad3e2f6c4dc6ef7788c481a5.jpg: 416x416 1 face, 7.9ms\n",
            "image 104/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-148_jpg.rf.dea78d2a473d7930a87bdfeb48150fca.jpg: 416x416 1 face, 7.4ms\n",
            "image 105/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-149_jpg.rf.6bb15e4aee0313e39b8765d482264b34.jpg: 416x416 2 faces, 7.4ms\n",
            "image 106/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-20_jpg.rf.549b9336d914405e0b7cae40ffe18412.jpg: 416x416 2 faces, 7.4ms\n",
            "image 107/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-30_jpg.rf.dbd5c9e24ff51be7dffe9dcf1e434a12.jpg: 416x416 2 faces, 7.4ms\n",
            "image 108/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-33_jpg.rf.3d8117fe60612d0ccbb96ac3b54f0565.jpg: 416x416 2 faces, 7.4ms\n",
            "image 109/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-39_jpg.rf.8106435863dfa37d1c8ebeffef20a425.jpg: 416x416 2 faces, 7.4ms\n",
            "image 110/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-3_jpg.rf.bb42d56b7cc101bd41a13e2530824a0c.jpg: 416x416 1 face, 7.4ms\n",
            "image 111/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-49_jpg.rf.14d5cdf89d249c3a5a53d514565bc713.jpg: 416x416 2 faces, 7.4ms\n",
            "image 112/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-92_jpg.rf.655bb88ef65428d936f57c4df4c5485d.jpg: 416x416 1 face, 7.4ms\n",
            "image 113/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/gravitate-faces_mp4-9_jpg.rf.39664a1dcb2416de37c19ef3af2c0c14.jpg: 416x416 1 face, 7.4ms\n",
            "image 114/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/hand-signs-for-alex_mp4-57_jpg.rf.0174af29dfda18f2fb75f4feeaf39a25.jpg: 416x416 1 face, 7.4ms\n",
            "image 115/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/istockphoto-1270066655-612x612_jpg.rf.59980d6953f060cb343628b0ce132a2d.jpg: 416x416 3 faces, 7.4ms\n",
            "image 116/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-10_jpg.rf.9f096c5316d747e4b6694483ebdddab8.jpg: 416x416 1 face, 7.4ms\n",
            "image 117/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-21_jpg.rf.3d816c05a73bb7a333690a6349a76e0c.jpg: 416x416 1 face, 7.4ms\n",
            "image 118/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-36_jpg.rf.e73aa0f4a48278767e0f5ffeaadd2fd8.jpg: 416x416 1 face, 7.4ms\n",
            "image 119/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-38_jpg.rf.3b459895ac4c9addaf28ef5ae8ccbf73.jpg: 416x416 1 face, 7.4ms\n",
            "image 120/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-50_jpg.rf.b4afd2716df4211f7290277abe5175f1.jpg: 416x416 1 face, 7.4ms\n",
            "image 121/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-56_jpg.rf.dec87ff66c920f4db27c20c20e1da673.jpg: 416x416 1 face, 7.4ms\n",
            "image 122/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-6_jpg.rf.1746a5fcaa6267c2aafd21869f8fd6bf.jpg: 416x416 1 face, 7.4ms\n",
            "image 123/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-71_jpg.rf.723a88ca9dcd0334f850de28f0d98375.jpg: 416x416 1 face, 7.4ms\n",
            "image 124/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-no-mask_mov-80_jpg.rf.6f7fb84cbf719628043c530307321b3a.jpg: 416x416 1 face, 7.4ms\n",
            "image 125/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632931966866_png_jpg.rf.f5d9be6dc97e583bd8ce44ff85dcb367.jpg: 416x416 3 faces, 7.4ms\n",
            "image 126/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632932097427_png_jpg.rf.19217f88e9c977e195dfe74b96604f8c.jpg: 416x416 3 faces, 7.4ms\n",
            "image 127/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632932267592_png_jpg.rf.06a4c51e15046ab182b54069f7c9e844.jpg: 416x416 1 face, 7.4ms\n",
            "image 128/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632933579037_png_jpg.rf.f0b73c1dcb02cd5c4ff581509fea4cac.jpg: 416x416 2 faces, 7.4ms\n",
            "image 129/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632937520400_png_jpg.rf.2b7acdc74125ac087fa9c53c9b15e946.jpg: 416x416 1 face, 7.4ms\n",
            "image 130/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632937587369_png_jpg.rf.41a691b91a580ea1f8a0b7ed5e43f52b.jpg: 416x416 1 face, 7.4ms\n",
            "image 131/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632937711552_png_jpg.rf.a7b25c3ea90926461bca2eba1987cbe1.jpg: 416x416 1 face, 7.4ms\n",
            "image 132/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632937724099_png_jpg.rf.ba674a1abd2e9b6ed7f3c4250b53a776.jpg: 416x416 1 face, 7.4ms\n",
            "image 133/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632937831241_png_jpg.rf.a1e2acace9b5996d239fb3303ab021d5.jpg: 416x416 1 face, 7.4ms\n",
            "image 134/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632937938545_png_jpg.rf.e04f6aa6fe0df71140cd02e68ad97a24.jpg: 416x416 1 face, 7.4ms\n",
            "image 135/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632938044674_png_jpg.rf.566c57b774194f0a4b528b28131bf86c.jpg: 416x416 1 face, 7.4ms\n",
            "image 136/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632938112706_png_jpg.rf.db469d2ea8744f383ab5987c1fa3ff0a.jpg: 416x416 1 face, 7.4ms\n",
            "image 137/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632938125416_png_jpg.rf.c49efac7a67342ba7ff0be808dbbd14a.jpg: 416x416 1 face, 7.4ms\n",
            "image 138/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632939814594_png_jpg.rf.d4b8f3ff3ad0000cbfd83e588524aac1.jpg: 416x416 1 face, 7.4ms\n",
            "image 139/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask-wearing-1632939984654_png_jpg.rf.06d659254a36b6439e813c986a1f92e9.jpg: 416x416 1 face, 7.4ms\n",
            "image 140/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask_mov-10_jpg.rf.7cc98cdfab565fe9e06a2f9924c7fbac.jpg: 416x416 (no detections), 7.4ms\n",
            "image 141/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mask_mov-14_jpg.rf.8393de220903f407e668f2608d8bba1f.jpg: 416x416 1 face, 7.4ms\n",
            "image 142/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/mo-justin-mask-NoMask_mov-1_jpg.rf.54e05b7c18a5b50bc22fa981d3b87f13.jpg: 416x416 2 faces, 7.4ms\n",
            "image 143/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/no-mask_mov-17_jpg.rf.a76732a9ddfc1b6292dc954692ed816f.jpg: 416x416 1 face, 7.4ms\n",
            "image 144/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/no-mask_mov-19_jpg.rf.1d38ec598b5acf9d381ce20d9c61b77b.jpg: 416x416 1 face, 7.4ms\n",
            "image 145/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/no-mask_mov-21_jpg.rf.35a875082bf3e9082619322c3e85066e.jpg: 416x416 1 face, 7.4ms\n",
            "image 146/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/no-mask_mov-25_jpg.rf.71dda9867d39929ca73b17973963f50b.jpg: 416x416 1 face, 7.4ms\n",
            "image 147/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/no-mask_mov-2_jpg.rf.6a1dc75296f4adeec21f8f6a16960ebf.jpg: 416x416 1 face, 7.4ms\n",
            "image 148/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/no-mask_mov-3_jpg.rf.a1c6a9755167c0c4c9636023af5a4c15.jpg: 416x416 1 face, 7.4ms\n",
            "image 149/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/no-mask_mov-5_jpg.rf.da8d5a2bd20d0ac2f0d2761f2bc3bbb7.jpg: 416x416 1 face, 7.4ms\n",
            "image 150/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/phplpE73q_jpg.rf.d6e6b502a404147028057743494accc3.jpg: 416x416 41 faces, 7.4ms\n",
            "image 151/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/r1000019q679o5611r7_jpg.rf.5e721d6d275c95f3361a0bbb687cc412.jpg: 416x416 1 face, 7.4ms\n",
            "image 152/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/r1p00017o8357s6sno6_jpg.rf.9ae86a05a908ae42017f843eeeeeb6af.jpg: 416x416 1 face, 7.4ms\n",
            "image 153/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/rally-against-an-anti-mask-law-meant-to-deter-anti-government-protesters-in-hong-kong-china-shutterstock-editorial-10435716z_jpg.rf.bce5ba5dd5d3babd3be49aa8eb1d498a.jpg: 416x416 11 faces, 7.4ms\n",
            "image 154/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/roboflow-44_jpg.rf.b3cf323e5d89309bd1b41146f9cf7fb6.jpg: 416x416 2 faces, 7.4ms\n",
            "image 155/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/sdfsdfsfff_jpg.rf.9599abc2e969781544ca60805ab71c5d.jpg: 416x416 1 face, 7.4ms\n",
            "image 156/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/stsciRq_png_jpg.rf.b2d1010d3f4a831b60faf9e994ab1f12.jpg: 416x416 1 face, 7.4ms\n",
            "image 157/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/tryxge43ecaa1_jpg.rf.ff9dab4f9f2438735e6a06fef42075bd.jpg: 416x416 1 license, 7.4ms\n",
            "image 158/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/u4wyzxa29oka1_jpg.rf.7dd76ec04e2806e71091560beed00800.jpg: 416x416 1 license, 7.4ms\n",
            "image 159/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/unnamed_jpg.rf.de5e3b1e57412b902f4fbce5ca200a24.jpg: 416x416 12 faces, 7.4ms\n",
            "image 160/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/uvyd7aoqg9ba1_jpg.rf.149d0b9722baafd500ef5a644f4b9bba.jpg: 416x416 1 license, 7.4ms\n",
            "image 161/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/w1240-p16x9-0e48e0098f6e832f27d8b581b33bbc72b9967a63_jpg.rf.9706ac063045d50d4e0647fdbdb1d23e.jpg: 416x416 2 faces, 7.4ms\n",
            "image 162/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/wqwpkf6fw4ha1_jpg.rf.46f40c92078859aa53f34bff3aba7b77.jpg: 416x416 1 license, 7.4ms\n",
            "image 163/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/x3ideauqz6ma1_jpg.rf.6dca314ae1d79e9e9a40dd2362731da2.jpg: 416x416 1 license, 7.4ms\n",
            "image 164/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/xg9o2ck79coa1_jpg.rf.3818284bce37e041c3c2e37dbbf825b8.jpg: 416x416 1 license, 7.4ms\n",
            "image 165/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/y2exv0huy1ma1_jpg.rf.88968f9ae4dcffe375bf8c28692323cd.jpg: 416x416 1 license, 7.4ms\n",
            "image 166/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/yfrjkwh4tbka1_jpg.rf.902fa72cb4fadd0c53a93bbc4382b25c.jpg: 416x416 (no detections), 7.4ms\n",
            "image 167/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-12_jpg.rf.f3569f73fc4a618e7058732f6e8051d2.jpg: 416x416 (no detections), 7.4ms\n",
            "image 168/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-16_jpg.rf.948c2a3e568b3cac8d14dce928c7fedd.jpg: 416x416 (no detections), 7.4ms\n",
            "image 169/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-43_jpg.rf.1743bb0c242cc5cc4f28899ef03c5e4c.jpg: 416x416 (no detections), 7.4ms\n",
            "image 170/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-46_jpg.rf.86e07c090f3d95908ff964d55f7e637c.jpg: 416x416 (no detections), 7.4ms\n",
            "image 171/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-72_jpg.rf.e96e77e92adf5c886496b2fd44eef4eb.jpg: 416x416 (no detections), 7.4ms\n",
            "image 172/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-74_jpg.rf.fc6b3ce73a8d59f7e94da5c906da3306.jpg: 416x416 (no detections), 7.4ms\n",
            "image 173/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-75_jpg.rf.5c353d53964791c407a0795f9d951cf3.jpg: 416x416 (no detections), 7.4ms\n",
            "image 174/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-76_jpg.rf.f29689010db4bfd41307cb8046085a0f.jpg: 416x416 (no detections), 7.4ms\n",
            "image 175/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-80_jpg.rf.cfeb295c2321703f70d89bff304b5623.jpg: 416x416 (no detections), 7.4ms\n",
            "image 176/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-83_jpg.rf.e26af26174f4ef7a8f3274b1545071d1.jpg: 416x416 (no detections), 7.4ms\n",
            "image 177/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/youtube-86_jpg.rf.bf5630b01c80dc5208ed7e7e3d609789.jpg: 416x416 (no detections), 7.5ms\n",
            "image 178/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/zpzdoej1oxfa1_jpg.rf.38d50b2d687beb07ec9de103588c50b0.jpg: 416x416 1 license, 7.4ms\n",
            "image 179/179 /content/yolov5/YOLOv5-ObjectDetection/test/images/zwa92s97xzaa1_jpg.rf.02519ba28c47a3b37f6102a0bd294c43.jpg: 416x416 1 license, 7.4ms\n",
            "Speed: 0.3ms pre-process, 7.6ms inference, 2.5ms NMS per image at shape (1, 3, 416, 416)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect-1.py --weights {PATH}/best.pt --img 416 --conf 0.7 --source {PATH}/test/images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.7 --source {dataset.location}/test/images"
      ],
      "metadata": {
        "id": "rtB05_Njyq3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbUn4_b9GCKO"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading The Cropped Images Folders"
      ],
      "metadata": {
        "id": "GvzLqlQo2DkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/Zip_Files'):\n",
        "  !mkdir {'/content/Zip_Files'}"
      ],
      "metadata": {
        "id": "BM_0zHZR3jpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/Zip_Files/Face.zip\" \"/content/yolov5/Cropped_Imgs/Face\"\n",
        "!zip -r \"/content/Zip_Files/License.zip\" \"/content/yolov5/Cropped_Imgs/License\"\n"
      ],
      "metadata": {
        "id": "VM2rNzye2DCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Video\n"
      ],
      "metadata": {
        "id": "KcSy4QjIZuoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for displaying og video\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_video(video):\n",
        "    fig = plt.figure(figsize=(3,3))  #Display size specification\n",
        "\n",
        "    mov = []\n",
        "    for i in range(len(video)):  #Append videos one by one to mov\n",
        "        img = plt.imshow(video[i], animated=True)\n",
        "        plt.axis('off')\n",
        "        mov.append([img])\n",
        "\n",
        "    #Animation creation\n",
        "    anime = animation.ArtistAnimation(fig, mov, interval=50, repeat_delay=1000)\n",
        "\n",
        "    plt.close()\n",
        "    return anime"
      ],
      "metadata": {
        "id": "wFrV3nigZxx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = imageio.mimread('/content/yolov5/runs/detect/exp4/video2.mp4',memtest=False)  #Loading video\n",
        "#video = [resize(frame, (256, 256))[..., :3] for frame in video]    #Size adjustment (if necessary)\n",
        "HTML(display_video(video).to_html5_video())  #Inline video display in HTML5"
      ],
      "metadata": {
        "id": "Btv308AqZzTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yNveqeA1KXGy"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}